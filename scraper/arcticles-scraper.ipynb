{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping \n",
    "## Import Libraries\n",
    "We start by importing all the needed libraries for scraping news data and then checking if the versions match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 1.6.1\n",
      "pandas version: 2.2.3\n",
      "seaborn version: 0.13.2\n",
      "requests version: 2.31.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas\n",
    "import seaborn\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"scikit-learn version:\", sklearn.__version__)     # 1.6.1\n",
    "print(\"pandas version:\", pandas.__version__)            # 2.2.3\n",
    "print(\"seaborn version:\", seaborn.__version__)          # 0.13.2\n",
    "print(\"requests version:\", requests.__version__)        # 2.31.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Details\n",
    "I plan to examine each article's:\n",
    "- title\n",
    "- link\n",
    "- source\n",
    "- author (if available)\n",
    "- publication date\n",
    "- text/content\n",
    "- \"Fake\" (0) or \"Real\" (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truthful websites\n",
    "### BBC\n",
    "I believe that starting with the BBC would be the easiest, as I did my research and found out that they were ranked as the most trusted news source by Americans. Even though we live in Europe, I choose to believe this statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to bbc_news_articles_v1.csv\n"
     ]
    }
   ],
   "source": [
    "bbc_url = 'https://www.bbc.com'\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # article text\n",
    "    article_text = ''\n",
    "    article_body = soup.find('article')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')  #find all p tags\n",
    "        for p in paragraphs:\n",
    "            article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # author\n",
    "    author_tag = soup.find('span', class_='sc-b42e7a8f-7 kItaYD')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else ''\n",
    "\n",
    "    # publication date\n",
    "    time_tag = soup.find('time', {'datetime': True})\n",
    "    publication_date = time_tag['datetime'] if time_tag else ''\n",
    "\n",
    "    return article_text, author, publication_date\n",
    "\n",
    "def scrape_news():\n",
    "    url = f'{bbc_url}/news'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles = []\n",
    "    for article in soup.find_all('a', class_='sc-2e6baa30-0 gILusN'):\n",
    "        title_element = article.find('h2', class_='sc-87075214-3 eywmDE')\n",
    "        if title_element:\n",
    "            title = title_element.get_text(strip=True)\n",
    "            href = article['href']\n",
    "\n",
    "            # check if href is a full URL or relative path\n",
    "            if href.startswith('http'):\n",
    "                link = href\n",
    "            else:\n",
    "                link = f\"{bbc_url}{href}\"\n",
    "\n",
    "            # additional details\n",
    "            article_text, author, publication_date = scrape_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'source': 'BBC',\n",
    "                'author': author,\n",
    "                'date': publication_date,\n",
    "                'text': article_text,\n",
    "                'classification': 1\n",
    "            })\n",
    "\n",
    "    return articles\n",
    "\n",
    "news_articles = scrape_news()\n",
    "\n",
    "# save to CSV\n",
    "df = pandas.DataFrame(news_articles)\n",
    "df.to_csv(\"bbc_news_articles_v1.csv\", index=False)\n",
    "\n",
    "print(\"Saved to bbc_news_articles_v1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake websites\n",
    "\n",
    "For now, I decided to generateb fake data for the fake news. As I looked extensively into datasets with fake news and most of them are 6-7 years old, which might not be still relevant. At this state of the project - iteration 0 - I decided to go with this fake data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sort marriage amount.</td>\n",
       "      <td>https://www.graves-lopez.com/personal-movement</td>\n",
       "      <td>Scott Inc News</td>\n",
       "      <td>James Craig</td>\n",
       "      <td>3/18/2025</td>\n",
       "      <td>Who force series movement tax will specific. B...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Shoulder agent say per enough.</td>\n",
       "      <td>http://www.ritter-daniel.com/his-bring-only</td>\n",
       "      <td>Larson Ltd News</td>\n",
       "      <td>Victor Snow</td>\n",
       "      <td>2/11/2022</td>\n",
       "      <td>Behind enjoy they trip theory piece season. Ah...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perform same tough risk authority store.</td>\n",
       "      <td>https://www.fernandez.net/thus-item-executive</td>\n",
       "      <td>Hanson LLC News</td>\n",
       "      <td>Alan Lewis</td>\n",
       "      <td>4/2/2024</td>\n",
       "      <td>Card whom history position learn leave rate. R...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Specific with agency.</td>\n",
       "      <td>https://scott-gates.info/indeed-vote-tend</td>\n",
       "      <td>Keller-Brewer News</td>\n",
       "      <td>Sarah Turner</td>\n",
       "      <td>2/6/2024</td>\n",
       "      <td>Manager bar reduce. Sing individual may floor ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>A TV within reduce sort concern different meas...</td>\n",
       "      <td>https://www.fleming.com/religious-state-my</td>\n",
       "      <td>Maddox-Anderson News</td>\n",
       "      <td>Tamara Rodriguez</td>\n",
       "      <td>9/2/2024</td>\n",
       "      <td>It open billion democratic. Partner activity s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "25                              Sort marriage amount.   \n",
       "24                     Shoulder agent say per enough.   \n",
       "1            Perform same tough risk authority store.   \n",
       "37                              Specific with agency.   \n",
       "27  A TV within reduce sort concern different meas...   \n",
       "\n",
       "                                              link                source  \\\n",
       "25  https://www.graves-lopez.com/personal-movement        Scott Inc News   \n",
       "24     http://www.ritter-daniel.com/his-bring-only       Larson Ltd News   \n",
       "1    https://www.fernandez.net/thus-item-executive       Hanson LLC News   \n",
       "37       https://scott-gates.info/indeed-vote-tend    Keller-Brewer News   \n",
       "27      https://www.fleming.com/religious-state-my  Maddox-Anderson News   \n",
       "\n",
       "              author       date  \\\n",
       "25       James Craig  3/18/2025   \n",
       "24       Victor Snow  2/11/2022   \n",
       "1         Alan Lewis   4/2/2024   \n",
       "37      Sarah Turner   2/6/2024   \n",
       "27  Tamara Rodriguez   9/2/2024   \n",
       "\n",
       "                                                 text  classification  \n",
       "25  Who force series movement tax will specific. B...               0  \n",
       "24  Behind enjoy they trip theory piece season. Ah...               0  \n",
       "1   Card whom history position learn leave rate. R...               0  \n",
       "37  Manager bar reduce. Sing individual may floor ...               0  \n",
       "27  It open billion democratic. Partner activity s...               0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_dataset = pandas.read_csv('fake_news_articles_v1.csv')\n",
    "fake_dataset.sample(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
