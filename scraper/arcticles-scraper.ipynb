{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping ‚õèÔ∏è\n",
    "\n",
    "Let's begin by importing the libraries and checking their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 1.6.1\n",
      "pandas version: 2.2.3\n",
      "seaborn version: 0.13.2\n",
      "requests version: 2.31.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas\n",
    "import seaborn\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"scikit-learn version:\", sklearn.__version__)     # 1.6.1\n",
    "print(\"pandas version:\", pandas.__version__)            # 2.2.3\n",
    "print(\"seaborn version:\", seaborn.__version__)          # 0.13.2\n",
    "print(\"requests version:\", requests.__version__)        # 2.31.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Details üîé\n",
    "I will examine each article's \n",
    "- Title \n",
    "- Link \n",
    "- Author (if available)\n",
    "- Publication Date\n",
    "- Content/text\n",
    "- Categorize it as \"Fake\" (0) or \"Real\" (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real News ‚úÖ \n",
    "### BBC üåê\n",
    "\n",
    "I believe that starting with the BBC would be the most straightforward option, as my research shows they are ranked as the most trusted news source by Americans. Even though we live in Europe, I choose to trust this statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved.\n"
     ]
    }
   ],
   "source": [
    "bbc_url = 'https://www.bbc.com'\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # article text\n",
    "    article_text = ''\n",
    "    article_body = soup.find('article')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')  #find all p tags\n",
    "        for p in paragraphs:\n",
    "            article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # author\n",
    "    author_tag = soup.find('span', class_='sc-b42e7a8f-7 kItaYD')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else ''\n",
    "\n",
    "    # publication date\n",
    "    time_tag = soup.find('time', {'datetime': True})\n",
    "    publication_date = time_tag['datetime'] if time_tag else ''\n",
    "\n",
    "    return article_text, author, publication_date\n",
    "\n",
    "def scrape_news():\n",
    "    url = f'{bbc_url}/news'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles = []\n",
    "    for article in soup.find_all('a', class_='sc-2e6baa30-0 gILusN'):\n",
    "        title_element = article.find('h2', class_='sc-87075214-3 eywmDE')\n",
    "        if title_element:\n",
    "            title = title_element.get_text(strip=True)\n",
    "            href = article['href']\n",
    "\n",
    "            # check if href is a full URL or relative path\n",
    "            if href.startswith('http'):\n",
    "                link = href\n",
    "            else:\n",
    "                link = f\"{bbc_url}{href}\"\n",
    "\n",
    "            # additional details\n",
    "            article_text, author, publication_date = scrape_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'source': 'BBC',\n",
    "                'journalist': author,\n",
    "                'date': publication_date,\n",
    "                'content': article_text,\n",
    "                'classification': 1 #1 for real news\n",
    "            })\n",
    "\n",
    "    return articles\n",
    "\n",
    "news_articles = scrape_news()\n",
    "\n",
    "# save to CSV\n",
    "df = pandas.DataFrame(news_articles)\n",
    "df.to_csv(\"bbc_news_articles_v6.csv\", index=False)\n",
    "\n",
    "print(\"Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News ‚ùå\n",
    "### The Onion üßÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and saved to 'onion_news_articles.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "onion_url = 'https://theonion.com'\n",
    "\n",
    "def scrape_onion_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract article text\n",
    "    article_text = ''\n",
    "    article_body = soup.find('div', class_='entry-content single-post-content single-post-content--has-watermark wp-block-post-content has-echo-font-size is-layout-flow wp-block-post-content-is-layout-flow')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # Extract publication date\n",
    "    publication_date = ''\n",
    "    time_tag = soup.find('time')\n",
    "    if time_tag and 'datetime' in time_tag.attrs:\n",
    "        publication_date = time_tag['datetime']\n",
    "\n",
    "    return article_text, publication_date\n",
    "\n",
    "def scrape_onion_news(pages=10):\n",
    "    articles = []\n",
    "    page = 1\n",
    "\n",
    "    while page <= pages:  # Limit to the specified number of pages\n",
    "        url = f'{onion_url}/news/page/{page}/'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Select article containers\n",
    "        article_count = 0\n",
    "        for article in soup.find_all('h3', class_='has-link-color wp-elements-aad9b6425bfbbfe81f8771ca6f420d00 wp-block-post-title has-text-color has-primary-2-color has-rocky-condensed-font-family'):\n",
    "            title = article.get_text(strip=True)\n",
    "            link_tag = article.find('a')\n",
    "            href = link_tag['href'] if link_tag else ''\n",
    "\n",
    "            # Ensure the link is a full URL\n",
    "            link = href if href.startswith('http') else f\"{onion_url}{href}\"\n",
    "\n",
    "            # Scrape article details\n",
    "            article_text, publication_date = scrape_onion_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'source': 'The Onion',\n",
    "                'journalist': '',  # Authors are typically not listed\n",
    "                'date': publication_date,\n",
    "                'content': article_text,\n",
    "                'classification': 0  # Mark as fake news\n",
    "            })\n",
    "            article_count += 1\n",
    "\n",
    "        # Break if no articles are found on the current page\n",
    "        if article_count == 0:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Scrape 5 pages\n",
    "onion_articles = scrape_onion_news(pages=10)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(onion_articles)\n",
    "df.to_csv(\"onion_news_articles.csv\", index=False)\n",
    "\n",
    "print(\"Scraping completed and saved to 'onion_news_articles.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The People's Voice üó£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and saved to 'peoples_voice_articles.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "base_url = 'https://thepeoplesvoice.tv'\n",
    "\n",
    "# Function to scrape details from an article\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract article text\n",
    "    article_text = ''\n",
    "    article_body = soup.find('div', class_='entry-content clearfix')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # Extract publication date\n",
    "    publication_date = ''\n",
    "    date_span = soup.find('span', class_='entry-meta-date updated')\n",
    "    if date_span:\n",
    "        date_link = date_span.find('a')\n",
    "        if date_link:\n",
    "            publication_date = date_link.get_text(strip=True)\n",
    "\n",
    "    # Extract author\n",
    "    author = ''\n",
    "    author_span = soup.find('span', class_='entry-meta-author author vcard')\n",
    "    if author_span:\n",
    "        author_link = author_span.find('a', class_='fn')\n",
    "        if author_link:\n",
    "            author = author_link.get_text(strip=True)\n",
    "\n",
    "    return article_text, publication_date, author\n",
    "\n",
    "# Function to scrape articles from multiple pages\n",
    "\n",
    "def scrape_news(pages=10):\n",
    "    articles = []\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f'{base_url}/category/news/page/{page}/'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Select article containers\n",
    "        article_count = 0\n",
    "        for article in soup.find_all('h3', class_='entry-title mh-posts-list-title'):\n",
    "            title = article.get_text(strip=True)\n",
    "            link_tag = article.find('a')\n",
    "            href = link_tag['href'] if link_tag else ''\n",
    "\n",
    "            # Ensure the link is a full URL\n",
    "            link = href if href.startswith('http') else f\"{base_url}{href}\"\n",
    "\n",
    "            # Scrape article details\n",
    "            article_text, publication_date, author = scrape_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'source': \"The People's Voice\",\n",
    "                'journalist': author,\n",
    "                'date': publication_date,\n",
    "                'content': article_text,\n",
    "                'classification': 0  # Mark as fake news\n",
    "            })\n",
    "            article_count += 1\n",
    "\n",
    "        # Break if no articles are found on the current page\n",
    "        if article_count == 0:\n",
    "            break\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Scrape 5 pages\n",
    "news_articles = scrape_news(pages=10)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(news_articles)\n",
    "df.to_csv(\"peoples_voice_articles.csv\", index=False)\n",
    "\n",
    "print(\"Scraping completed and saved to 'peoples_voice_articles.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
