{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping ‚õèÔ∏è - Iteration 1\n",
    "The goal of this project is to distinguish between false and truthful information. To achieve this, it's essential to gather a solid dataset of recent true and fake news for training the model.\n",
    "\n",
    "First, let's start by importing the necessary libraries and checking their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 1.6.1\n",
      "pandas version: 2.2.3\n",
      "seaborn version: 0.13.2\n",
      "requests version: 2.31.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas\n",
    "import seaborn\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import cloudscraper\n",
    "import time\n",
    "\n",
    "print(\"scikit-learn version:\", sklearn.__version__)     # 1.6.1\n",
    "print(\"pandas version:\", pandas.__version__)            # 2.2.3\n",
    "print(\"seaborn version:\", seaborn.__version__)          # 0.13.2\n",
    "print(\"requests version:\", requests.__version__)        # 2.31.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Details üîé\n",
    "The data I want to derive from each article is the following:\n",
    "- Title \n",
    "- URL\n",
    "- Source \n",
    "- Journalist(s) (if available)\n",
    "- Published Date\n",
    "- Content of article\n",
    "- Category (politics, sports, etc.)\n",
    "- Label, which is either \"Fake\" (0) or \"Real\" (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News ‚ùå\n",
    "As the internet evolves, most fake news websites are taken down or become unavailable. Therefore, the only ones I was able to find are:\n",
    "- https://theonion.com/ (done)\n",
    "- https://thepeoplesvoice.tv/ (done)\n",
    "- https://www.theinteldrop.org/ (done)\n",
    "- https://spacexmania.com/ (done)\n",
    "- beforeitsnews.com\n",
    "- naturalnews.com\n",
    "- newspunch.com\n",
    "- realrawnews.com\n",
    "- worldnewsdailyreport.com\n",
    "- thegatewaypundit.com\n",
    "- babylonbee.com\n",
    "- dailysquib.co.uk\n",
    "\n",
    "### <a href=\"https://theonion.com/\">The Onion</a> üßÖ\n",
    "The Onion is an American digital media company and newspaper organization that publishes satirical articles on international, national and local news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "onion_url = 'https://theonion.com'\n",
    "\n",
    "def scrape_onion_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # article text\n",
    "    article_text = ''\n",
    "    article_body = soup.find('div', class_='entry-content single-post-content single-post-content--has-watermark wp-block-post-content has-echo-font-size is-layout-flow wp-block-post-content-is-layout-flow')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # date\n",
    "    publication_date = ''\n",
    "    time_tag = soup.find('time')\n",
    "    if time_tag and 'datetime' in time_tag.attrs:\n",
    "        publication_date = time_tag['datetime']\n",
    "\n",
    "    return article_text, publication_date\n",
    "\n",
    "def scrape_onion_news(pages=10):\n",
    "    articles = []\n",
    "    page = 1\n",
    "\n",
    "    while page <= pages:\n",
    "        url = f'{onion_url}/news/page/{page}/'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # article containers\n",
    "        article_count = 0\n",
    "        for article in soup.find_all('h3', class_='has-link-color wp-elements-aad9b6425bfbbfe81f8771ca6f420d00 wp-block-post-title has-text-color has-primary-2-color has-rocky-condensed-font-family'):\n",
    "            title = article.get_text(strip=True)\n",
    "            link_tag = article.find('a')\n",
    "            href = link_tag['href'] if link_tag else ''\n",
    "\n",
    "            # link is a full URL\n",
    "            link = href if href.startswith('http') else f\"{onion_url}{href}\"\n",
    "\n",
    "            # scrape article details\n",
    "            article_text, publication_date = scrape_onion_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'Title': title,\n",
    "                'URL': link,\n",
    "                'Source': 'The Onion',\n",
    "                'Journalist(s)': '',  # authors are not listed\n",
    "                'Published Date': publication_date,\n",
    "                'Content': article_text,\n",
    "                'Label': 0  # fake news\n",
    "            })\n",
    "            article_count += 1\n",
    "\n",
    "        # break if no articles are found on the current page\n",
    "        if article_count == 0:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return articles\n",
    "\n",
    "# 10 pages\n",
    "onion_articles = scrape_onion_news(pages=10)\n",
    "\n",
    "# save to csv\n",
    "df = pandas.DataFrame(onion_articles)\n",
    "df.to_csv(\"the_onion_fake.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://thepeoplesvoice.tv\">The People's Voice</a> üîä \n",
    "Founded by Sean Adl-Tabatabai and Sinclair Treadway in 2014. It has published fake stories, such as \"claims that the Queen had threatened to abdicate if the UK voted against Brexit.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://thepeoplesvoice.tv'\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # article text\n",
    "    article_text = ''\n",
    "    article_body = soup.find('div', class_='entry-content clearfix')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # date\n",
    "    publication_date = ''\n",
    "    date_span = soup.find('span', class_='entry-meta-date updated')\n",
    "    if date_span:\n",
    "        date_link = date_span.find('a')\n",
    "        if date_link:\n",
    "            publication_date = date_link.get_text(strip=True)\n",
    "\n",
    "    # author\n",
    "    author = ''\n",
    "    author_span = soup.find('span', class_='entry-meta-author author vcard')\n",
    "    if author_span:\n",
    "        author_link = author_span.find('a', class_='fn')\n",
    "        if author_link:\n",
    "            author = author_link.get_text(strip=True)\n",
    "\n",
    "    return article_text, publication_date, author\n",
    "\n",
    "def scrape_news(pages=10):\n",
    "    articles = []\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f'{base_url}/category/news/page/{page}/'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # article containers\n",
    "        article_count = 0\n",
    "        for article in soup.find_all('h3', class_='entry-title mh-posts-list-title'):\n",
    "            title = article.get_text(strip=True)\n",
    "            link_tag = article.find('a')\n",
    "            href = link_tag['href'] if link_tag else ''\n",
    "\n",
    "            # link is a full URL\n",
    "            link = href if href.startswith('http') else f\"{base_url}{href}\"\n",
    "\n",
    "            # article details\n",
    "            article_text, publication_date, author = scrape_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'Title': title,\n",
    "                'URL': link,\n",
    "                'Source': \"The People's Voice\",\n",
    "                'Journalist(s)': author,\n",
    "                'Published Date': publication_date,\n",
    "                'Content': article_text,\n",
    "                'Label': 0  # fake news\n",
    "            })\n",
    "            article_count += 1\n",
    "\n",
    "        # break if no articles are found on the current page\n",
    "        if article_count == 0:\n",
    "            break\n",
    "\n",
    "    return articles\n",
    "\n",
    "# 10 pages\n",
    "news_articles = scrape_news(pages=10)\n",
    "\n",
    "# save to csv\n",
    "df = pandas.DataFrame(news_articles)\n",
    "df.to_csv(\"peoples_voice_fake.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://www.theinteldrop.org\">The Intel Drop</a>üíß\n",
    "The Intel Drop is a common Russian disinformation website.\t\n",
    "\n",
    "Headers and cloudscraper are used here to simulate a real browser and bypass anti-bot measures put in order to prevent the scraping. Headers improve the legitimacy of requests by incorporating details such as User-Agent and language preferences. On the other hand, cloudscraper assists in bypassing Cloudflare protections, including CAPTCHAs, while maintaining session consistency to ensure smooth data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://www.theinteldrop.org'\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Referer': base_url\n",
    "}\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    try:\n",
    "        response = scraper.get(link, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # skip p tags with this specific class\n",
    "        def is_valid_paragraph(p):\n",
    "            cls = p.get('class', [])\n",
    "            return ('has-text-align-center' not in cls and\n",
    "                    'has-text-color' not in cls and\n",
    "                    'has-medium-font-size' not in cls)\n",
    "\n",
    "        all_paragraphs = soup.select('div.entry-content p')\n",
    "        filtered_paragraphs = [p for p in all_paragraphs if is_valid_paragraph(p)]\n",
    "\n",
    "        # combine cleaned content\n",
    "        article_text = ' '.join(p.get_text(strip=True) for p in filtered_paragraphs).replace('\\u00a0', ' ').strip()\n",
    "\n",
    "        # date\n",
    "        date = soup.find('time', class_='entry-date published')\n",
    "        publication_date = date.get_text(strip=True) if date else ''\n",
    "\n",
    "        # author\n",
    "        author = ''\n",
    "        first_paragraph = next((p for p in filtered_paragraphs if p.find('strong') and p.find('em')), None)\n",
    "        if first_paragraph:\n",
    "            em_tag = first_paragraph.find('em')\n",
    "            if em_tag and \"by\" in em_tag.text.lower():\n",
    "                strong_tag = first_paragraph.find('strong')\n",
    "                if strong_tag:\n",
    "                    author = strong_tag.find('em').get_text(strip=True)\n",
    "\n",
    "        return article_text, publication_date, author\n",
    "    except Exception as e:\n",
    "        return '', '', ''\n",
    "\n",
    "def scrape_news(pages=2):\n",
    "    articles = []\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f'{base_url}/category/featured/page/{page}/'\n",
    "\n",
    "        response = scraper.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles_found = soup.find_all('h2', class_='penci-entry-title')\n",
    "\n",
    "        for article in articles_found:\n",
    "            title = article.get_text(strip=True)\n",
    "            link = article.find('a')['href']\n",
    "            if not link.startswith('http'):\n",
    "                link = f\"{base_url}{link}\"\n",
    "                \n",
    "            article_text, date, author = scrape_article_details(link)\n",
    "            articles.append({\n",
    "                'Title': title,\n",
    "                'URL': link,\n",
    "                'Source': \"The Intel Drop\",\n",
    "                'Journalist(s)': author,\n",
    "                'Published Date': date,\n",
    "                'Content': article_text,\n",
    "                'Label': 0 # fake news\n",
    "            })\n",
    "            time.sleep(1)  # delay between article requests\n",
    "\n",
    "    return articles\n",
    "\n",
    "# 2 pages\n",
    "news_articles = scrape_news(pages=2)\n",
    "\n",
    "# save to csv\n",
    "df = pandas.DataFrame(news_articles)\n",
    "df.to_csv(\"intel_drop_fake.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://spacexmania.com\">SpaceXMania</a> ü™ê\n",
    "Accused by Christopher Blair, the owner of The Last Line of Defense, of plagiarizing its stories via a generative AI model. The New York Times also found that many stories from SpaceXMania were produced by generative AI. The Australian Associated Press found that the site team used ChatGPT for writing content. Accused by academics of deceptively labeling clickbait content as satire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://spacexmania.com'\n",
    "\n",
    "category_url = f'{base_url}/category/satire/'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Referer': base_url\n",
    "}\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    try:\n",
    "        res = requests.get(link, headers=headers)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # date\n",
    "        date_tag = soup.find('time', class_='entry-date published')\n",
    "        publication_date = date_tag.get_text(strip=True) if date_tag else ''\n",
    "\n",
    "        # author\n",
    "        author_tag = soup.find('span', class_='author-name')\n",
    "        author = author_tag.get_text(strip=True) if author_tag else ''\n",
    "\n",
    "        # flatten all p tag content into a single string\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article_text = ' '.join(\n",
    "            p.get_text(strip=True).replace('\\n', ' ').replace('\\r', ' ')\n",
    "            for p in paragraphs if p.get_text(strip=True)\n",
    "        ).replace('\\u00a0', ' ').strip()\n",
    "\n",
    "        return article_text, publication_date, author\n",
    "    \n",
    "    except Exception as e:\n",
    "        return '', '', ''\n",
    "\n",
    "def scrape_spacexmania(max_articles=150):\n",
    "    articles = []\n",
    "    page = 1\n",
    "\n",
    "    while len(articles) < max_articles:\n",
    "        url = f'{category_url}page/{page}/'\n",
    "        try:\n",
    "            res = requests.get(url, headers=headers)\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            post_blocks = soup.find_all('h2', class_='entry-title')\n",
    "\n",
    "            for post in post_blocks:\n",
    "                if len(articles) >= max_articles:\n",
    "                    break\n",
    "\n",
    "                a_tag = post.find('a')\n",
    "                title = a_tag.get_text(strip=True)\n",
    "                link = a_tag['href']\n",
    "                if not link.startswith('http'):\n",
    "                    link = f\"{base_url}{link}\"\n",
    "\n",
    "                article_text, date, author = scrape_article_details(link)\n",
    "\n",
    "                articles.append({\n",
    "                    'Title': title,\n",
    "                    'URL': link,\n",
    "                    'Ssource': \"SpaceXMania\",\n",
    "                    'Journalist(s)': author,\n",
    "                    'Published Date': date,\n",
    "                    'Content': article_text,\n",
    "                    'Label': 0 # fake news\n",
    "                })\n",
    "                time.sleep(1)  # polite scraping delay\n",
    "\n",
    "        except Exception as e:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return articles\n",
    "\n",
    "# save 150 articles\n",
    "space_articles = scrape_spacexmania(max_articles=150)\n",
    "\n",
    "# save to csv\n",
    "df = pandas.DataFrame(space_articles)\n",
    "df.to_csv(\"spacexmania_fake.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real News ‚úÖ \n",
    "Opposite goes for the real news, it is much easier to find truthful websites:\n",
    "- https://www.bbc.com (done)\n",
    "\n",
    "### <a href=\"https://www.bbc.com\">BBC</a> üåê\n",
    "\n",
    "I believe that starting with the BBC would be the most straightforward option, as my research shows they are ranked as the most trusted news source by Americans. Even though we live in Europe, I choose to trust this statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "bbc_url = 'https://www.bbc.com'\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # article text\n",
    "    article_text = ''\n",
    "    article_body = soup.find('article')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')  #find all p tags\n",
    "        for p in paragraphs:\n",
    "            article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # author\n",
    "    author_tag = soup.find('span', class_='sc-b42e7a8f-7 kItaYD')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else ''\n",
    "\n",
    "    # publication date\n",
    "    time_tag = soup.find('time', {'datetime': True})\n",
    "    publication_date = time_tag['datetime'] if time_tag else ''\n",
    "\n",
    "    return article_text, author, publication_date\n",
    "\n",
    "def scrape_news():\n",
    "    url = f'{bbc_url}/news'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles = []\n",
    "    for article in soup.find_all('a', class_='sc-2e6baa30-0 gILusN'):\n",
    "        title_element = article.find('h2', class_='sc-87075214-3 eywmDE')\n",
    "        if title_element:\n",
    "            title = title_element.get_text(strip=True)\n",
    "            href = article['href']\n",
    "\n",
    "            # check if href is a full URL or relative path\n",
    "            if href.startswith('http'):\n",
    "                link = href\n",
    "            else:\n",
    "                link = f\"{bbc_url}{href}\"\n",
    "\n",
    "            # additional details\n",
    "            article_text, author, publication_date = scrape_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'Title': title,\n",
    "                'URL': link,\n",
    "                'Source': 'BBC',\n",
    "                'Journalist(s)': author,\n",
    "                'Published Date': publication_date,\n",
    "                'Content': article_text,\n",
    "                'Label': 1 # real news\n",
    "            })\n",
    "\n",
    "    return articles\n",
    "\n",
    "news_articles = scrape_news()\n",
    "\n",
    "# save to CSV\n",
    "df = pandas.DataFrame(news_articles)\n",
    "df.to_csv(\"bbc_06.04.25_real.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
