{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping ‚õèÔ∏è\n",
    "The goal of this project is to distinguish between false and truthful information. To achieve this, it is essential to gather a solid dataset of recent true and fake news for training the model.\n",
    "\n",
    "First, let's start by importing the necessary libraries and checking their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library versions:\n",
      "- pandas version: 2.2.3\n",
      "- sklearn version: 1.6.1\n",
      "- seaborn version: 0.13.2\n",
      "- requests version: 2.31.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas\n",
    "import sklearn\n",
    "import seaborn\n",
    "import requests\n",
    "import cloudscraper\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"Library versions:\")\n",
    "print(\"- pandas version:\", pandas.__version__)          # 2.2.3\n",
    "print(\"- sklearn version:\", sklearn.__version__)         # 1.6.1\n",
    "print(\"- seaborn version:\", seaborn.__version__)        # 0.13.2\n",
    "print(\"- requests version:\", requests.__version__)      # 2.31.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Details üîé\n",
    "The data I want to derive from each article is the following:\n",
    "- Title \n",
    "- URL\n",
    "- Source \n",
    "- Journalist(s) (if available)\n",
    "- Published Date\n",
    "- Content of the article\n",
    "- Category (Politics, Sports, etc.) - The data had to be categorised manually, as most of it had already been scraped.\n",
    "- Label: either \"Fake\" (0) or \"Real\" (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News ‚ùå\n",
    "As the internet evolves, most fake news websites are taken down or become unavailable. Therefore, the only ones I was able to find are:\n",
    "- https://theonion.com/\n",
    "- https://thepeoplesvoice.tv/\n",
    "- https://www.theinteldrop.org/\n",
    "- https://spacexmania.com/\n",
    "\n",
    "### <a href=\"https://theonion.com/\">The Onion</a> üßÖ\n",
    "The Onion is a well-known American satirical news outlet founded in 1988. It publishes humorous and exaggerated stories that parody real-world events and media narratives. Although its content is entirely fictional, it is clearly labeled as satire and is not intended to mislead. The Onion serves as a baseline for detecting intentionally fake yet transparent content that mimics news format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "onion_url = 'https://theonion.com'\n",
    "\n",
    "def scrape_onion_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # article text\n",
    "    article_text = ''\n",
    "    article_body = soup.find('div', class_='entry-content single-post-content single-post-content--has-watermark wp-block-post-content has-echo-font-size is-layout-flow wp-block-post-content-is-layout-flow')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # date\n",
    "    publication_date = ''\n",
    "    time_tag = soup.find('time')\n",
    "    if time_tag and 'datetime' in time_tag.attrs:\n",
    "        publication_date = time_tag['datetime']\n",
    "\n",
    "    return article_text, publication_date\n",
    "\n",
    "def scrape_onion_news(pages=10):\n",
    "    articles = []\n",
    "    page = 1\n",
    "\n",
    "    while page <= pages:\n",
    "        url = f'{onion_url}/news/page/{page}/'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # article containers\n",
    "        article_count = 0\n",
    "        for article in soup.find_all('h3', class_='has-link-color wp-elements-aad9b6425bfbbfe81f8771ca6f420d00 wp-block-post-title has-text-color has-primary-2-color has-rocky-condensed-font-family'):\n",
    "            title = article.get_text(strip=True)\n",
    "            link_tag = article.find('a')\n",
    "            href = link_tag['href'] if link_tag else ''\n",
    "\n",
    "            # link is a full URL\n",
    "            link = href if href.startswith('http') else f\"{onion_url}{href}\"\n",
    "\n",
    "            # scrape article details\n",
    "            article_text, publication_date = scrape_onion_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'Title': title,\n",
    "                'URL': link,\n",
    "                'Source': 'The Onion',\n",
    "                'Journalist(s)': '',  # authors are not listed\n",
    "                'Published Date': publication_date,\n",
    "                'Content': article_text,\n",
    "                'Label': 0  # fake news\n",
    "            })\n",
    "            article_count += 1\n",
    "\n",
    "        # break if no articles are found on the current page\n",
    "        if article_count == 0:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return articles\n",
    "\n",
    "# 10 pages\n",
    "onion_articles = scrape_onion_news(pages=10)\n",
    "\n",
    "# save to csv\n",
    "df = pandas.DataFrame(onion_articles)\n",
    "df.to_csv(\"the_onion_fake.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://thepeoplesvoice.tv\">The People's Voice</a> üîä \n",
    "This website is a prolific disinformation source known for publishing conspiracy theories, health misinformation and politically charged fake stories. In a 2017 analysis by BuzzFeed News, it ranked as the second-largest fake news site on Facebook based on engagement. It frequently spreads hoaxes and false claims without evidence and has been flagged by fact-checkers across multiple platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://thepeoplesvoice.tv'\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # article text\n",
    "    article_text = ''\n",
    "    article_body = soup.find('div', class_='entry-content clearfix')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')\n",
    "        for p in paragraphs:\n",
    "            article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # date\n",
    "    publication_date = ''\n",
    "    date_span = soup.find('span', class_='entry-meta-date updated')\n",
    "    if date_span:\n",
    "        date_link = date_span.find('a')\n",
    "        if date_link:\n",
    "            publication_date = date_link.get_text(strip=True)\n",
    "\n",
    "    # author\n",
    "    author = ''\n",
    "    author_span = soup.find('span', class_='entry-meta-author author vcard')\n",
    "    if author_span:\n",
    "        author_link = author_span.find('a', class_='fn')\n",
    "        if author_link:\n",
    "            author = author_link.get_text(strip=True)\n",
    "\n",
    "    return article_text, publication_date, author\n",
    "\n",
    "def scrape_news(pages=10):\n",
    "    articles = []\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f'{base_url}/category/news/page/{page}/'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # article containers\n",
    "        article_count = 0\n",
    "        for article in soup.find_all('h3', class_='entry-title mh-posts-list-title'):\n",
    "            title = article.get_text(strip=True)\n",
    "            link_tag = article.find('a')\n",
    "            href = link_tag['href'] if link_tag else ''\n",
    "\n",
    "            # link is a full URL\n",
    "            link = href if href.startswith('http') else f\"{base_url}{href}\"\n",
    "\n",
    "            # article details\n",
    "            article_text, publication_date, author = scrape_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'Title': title,\n",
    "                'URL': link,\n",
    "                'Source': \"The People's Voice\",\n",
    "                'Journalist(s)': author,\n",
    "                'Published Date': publication_date,\n",
    "                'Content': article_text,\n",
    "                'Label': 0  # fake news\n",
    "            })\n",
    "            article_count += 1\n",
    "\n",
    "        # break if no articles are found on the current page\n",
    "        if article_count == 0:\n",
    "            break\n",
    "\n",
    "    return articles\n",
    "\n",
    "# 10 pages\n",
    "news_articles = scrape_news(pages=10)\n",
    "\n",
    "# save to csv\n",
    "df = pandas.DataFrame(news_articles)\n",
    "df.to_csv(\"peoples_voice_fake.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://www.theinteldrop.org\">The Intel Drop</a>üíß\n",
    "The Intel Drop is widely regarded as a Russian disinformation platform. It has been linked to the dissemination of anti-Western propaganda, conspiracy theories and content aligned with Russian geopolitical interests. Its articles are often presented in a pseudo-journalistic tone, making them appear credible at first glance while subtly embedding misleading narratives.\n",
    "\n",
    "Headers and cloudscraper are used here to simulate a real browser and bypass anti-bot measures put in order to prevent the scraping. Headers improve the legitimacy of requests by incorporating details such as User-Agent and language preferences. On the other hand, cloudscraper assists in bypassing Cloudflare protections, including CAPTCHAs, while maintaining session consistency to ensure smooth data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://www.theinteldrop.org'\n",
    "\n",
    "scraper = cloudscraper.create_scraper()\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Referer': base_url\n",
    "}\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    try:\n",
    "        response = scraper.get(link, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # skip p tags with this specific class\n",
    "        def is_valid_paragraph(p):\n",
    "            cls = p.get('class', [])\n",
    "            return ('has-text-align-center' not in cls and\n",
    "                    'has-text-color' not in cls and\n",
    "                    'has-medium-font-size' not in cls)\n",
    "\n",
    "        all_paragraphs = soup.select('div.entry-content p')\n",
    "        filtered_paragraphs = [p for p in all_paragraphs if is_valid_paragraph(p)]\n",
    "\n",
    "        # combine cleaned content\n",
    "        article_text = ' '.join(p.get_text(strip=True) for p in filtered_paragraphs).replace('\\u00a0', ' ').strip()\n",
    "\n",
    "        # date\n",
    "        date = soup.find('time', class_='entry-date published')\n",
    "        publication_date = date.get_text(strip=True) if date else ''\n",
    "\n",
    "        # author\n",
    "        author = ''\n",
    "        first_paragraph = next((p for p in filtered_paragraphs if p.find('strong') and p.find('em')), None)\n",
    "        if first_paragraph:\n",
    "            em_tag = first_paragraph.find('em')\n",
    "            if em_tag and \"by\" in em_tag.text.lower():\n",
    "                strong_tag = first_paragraph.find('strong')\n",
    "                if strong_tag:\n",
    "                    author = strong_tag.find('em').get_text(strip=True)\n",
    "\n",
    "        return article_text, publication_date, author\n",
    "    except Exception as e:\n",
    "        return '', '', ''\n",
    "\n",
    "def scrape_news(pages=2):\n",
    "    articles = []\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f'{base_url}/category/featured/page/{page}/'\n",
    "\n",
    "        response = scraper.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        articles_found = soup.find_all('h2', class_='penci-entry-title')\n",
    "\n",
    "        for article in articles_found:\n",
    "            title = article.get_text(strip=True)\n",
    "            link = article.find('a')['href']\n",
    "            if not link.startswith('http'):\n",
    "                link = f\"{base_url}{link}\"\n",
    "                \n",
    "            article_text, date, author = scrape_article_details(link)\n",
    "            articles.append({\n",
    "                'Title': title,\n",
    "                'URL': link,\n",
    "                'Source': \"The Intel Drop\",\n",
    "                'Journalist(s)': author,\n",
    "                'Published Date': date,\n",
    "                'Content': article_text,\n",
    "                'Label': 0 # fake news\n",
    "            })\n",
    "            time.sleep(1)  # delay between article requests\n",
    "\n",
    "    return articles\n",
    "\n",
    "# 2 pages\n",
    "news_articles = scrape_news(pages=2)\n",
    "\n",
    "# save to csv\n",
    "df = pandas.DataFrame(news_articles)\n",
    "df.to_csv(\"intel_drop_fake.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://spacexmania.com\">SpaceXMania</a> üöÄ\n",
    "SpaceXMania is a website that began as a parody and pop culture site but has since blurred the line between satire and misinformation. According to The New York Times, many of its stories are generated by AI tools like ChatGPT and often lack clear satire disclaimers. Investigations by the Australian Associated Press and academic researchers suggest that the site intentionally labels AI-generated clickbait and fabricated content as ‚Äúsatire‚Äù to avoid accountability, making it a notable example of modern misinformation strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://spacexmania.com'\n",
    "\n",
    "category_url = f'{base_url}/category/satire/'\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Referer': base_url\n",
    "}\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    try:\n",
    "        res = requests.get(link, headers=headers)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # date\n",
    "        date_tag = soup.find('time', class_='entry-date published')\n",
    "        publication_date = date_tag.get_text(strip=True) if date_tag else ''\n",
    "\n",
    "        # author\n",
    "        author_tag = soup.find('span', class_='author-name')\n",
    "        author = author_tag.get_text(strip=True) if author_tag else ''\n",
    "\n",
    "        # flatten all p tag content into a single string\n",
    "        paragraphs = soup.find_all('p')\n",
    "        article_text = ' '.join(\n",
    "            p.get_text(strip=True).replace('\\n', ' ').replace('\\r', ' ')\n",
    "            for p in paragraphs if p.get_text(strip=True)\n",
    "        ).replace('\\u00a0', ' ').strip()\n",
    "\n",
    "        return article_text, publication_date, author\n",
    "    \n",
    "    except Exception as e:\n",
    "        return '', '', ''\n",
    "\n",
    "def scrape_spacexmania(max_articles=150):\n",
    "    articles = []\n",
    "    page = 1\n",
    "\n",
    "    while len(articles) < max_articles:\n",
    "        url = f'{category_url}page/{page}/'\n",
    "        \n",
    "        res = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        post_blocks = soup.find_all('h2', class_='entry-title')\n",
    "\n",
    "        for post in post_blocks:\n",
    "            if len(articles) >= max_articles:\n",
    "                break\n",
    "\n",
    "            a_tag = post.find('a')\n",
    "            title = a_tag.get_text(strip=True)\n",
    "            link = a_tag['href']\n",
    "            if not link.startswith('http'):\n",
    "                link = f\"{base_url}{link}\"\n",
    "\n",
    "            article_text, date, author = scrape_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'Title': title,\n",
    "                'URL': link,\n",
    "                'Ssource': \"SpaceXMania\",\n",
    "                'Journalist(s)': author,\n",
    "                'Published Date': date,\n",
    "                'Content': article_text,\n",
    "                'Label': 0 # fake news\n",
    "            })\n",
    "            time.sleep(1)  # polite scraping delay\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return articles\n",
    "\n",
    "# save 150 articles\n",
    "space_articles = scrape_spacexmania(max_articles=150)\n",
    "\n",
    "# save to csv\n",
    "df = pandas.DataFrame(space_articles)\n",
    "df.to_csv(\"spacexmania_fake.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real News ‚úÖ \n",
    "Opposite goes for the real news, it is much easier to find truthful websites:\n",
    "- https://www.bbc.com\n",
    "- https://apnews.com\n",
    "- https://techcrunch.com\n",
    "\n",
    "Unfortunately, some of the articles are behind a paywall, such as The New York Times, The Washington Post and Bloomberg. Consequently, I had to discard some of them, while others (Reuters) denied me access due to restrictions.\n",
    "\n",
    "### <a href=\"https://www.bbc.com\">BBC</a> üåê\n",
    "\n",
    "I believe that starting with the BBC would be the most straightforward option, as my research shows they are ranked as the most trusted news source by Americans. Even though we live in Europe, I choose to trust this statistic. \n",
    "\n",
    "Founded in 1922, the BBC is the world‚Äôs oldest national broadcaster and one of the most respected media organizations globally. It operates under a royal charter in the UK and is funded primarily by the public television license fee, which supports its editorial independence. The BBC is known for its rigorous editorial standards, in-depth investigative journalism and commitment to impartiality. It covers a wide spectrum of news topics from global politics to science and technology. Various media bias tracking platforms - including Media Bias/Fact Check and Ad Fontes Media - consistently rate the BBC as center-aligned, factual and minimally biased. Its reputation for reliability made it a natural choice for training the ‚Äúreal news‚Äù component of DeFactify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "bbc_url = 'https://www.bbc.com'\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # article text\n",
    "    article_text = ''\n",
    "    article_body = soup.find('article')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')  #find all p tags\n",
    "        for p in paragraphs:\n",
    "            article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # author\n",
    "    author_tag = soup.find('span', class_='sc-b42e7a8f-7 kItaYD')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else ''\n",
    "\n",
    "    # publication date\n",
    "    time_tag = soup.find('time', {'datetime': True})\n",
    "    publication_date = time_tag['datetime'] if time_tag else ''\n",
    "\n",
    "    return article_text, author, publication_date\n",
    "\n",
    "def scrape_news():\n",
    "    url = f'{bbc_url}/news'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles = []\n",
    "    for article in soup.find_all('a', class_='sc-2e6baa30-0 gILusN'):\n",
    "        title_element = article.find('h2', class_='sc-87075214-3 eywmDE')\n",
    "        if title_element:\n",
    "            title = title_element.get_text(strip=True)\n",
    "            href = article['href']\n",
    "\n",
    "            # check if href is a full URL or relative path\n",
    "            if href.startswith('http'):\n",
    "                link = href\n",
    "            else:\n",
    "                link = f\"{bbc_url}{href}\"\n",
    "\n",
    "            # additional details\n",
    "            article_text, author, publication_date = scrape_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'Title': title,\n",
    "                'URL': link,\n",
    "                'Source': 'BBC',\n",
    "                'Journalist(s)': author,\n",
    "                'Published Date': publication_date,\n",
    "                'Content': article_text,\n",
    "                'Label': 1 # real news\n",
    "            })\n",
    "\n",
    "    return articles\n",
    "\n",
    "news_articles = scrape_news()\n",
    "\n",
    "# save to csv\n",
    "df = pandas.DataFrame(news_articles)\n",
    "df.to_csv(\"bbc_06.04.25_real.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://apnews.com/\">AP News</a> ‚ô¶Ô∏è \n",
    "Provides unbiased, fact-based reporting. The Associated Press (AP) is a U.S. - based not-for-profit news agency founded in 1846. It is owned by its contributing newspapers, radio and television stations, all of which contribute stories to the AP and use material written by its staff journalists. The AP is renowned for its fast, accurate and unbiased news coverage across the globe. Its writing is widely syndicated and used as a gold standard in journalism training. The AP has won dozens of Pulitzer Prizes and maintains a strict code of ethics, making it an essential benchmark for factual, politically neutral reporting. It plays a critical role in ensuring the model does not mistake honest journalism for manipulated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://apnews.com\"\n",
    "\n",
    "CATEGORY_URLS = {\n",
    "    \"World News\": f\"{BASE_URL}/world-news\",\n",
    "    \"Politics\": f\"{BASE_URL}/politics\",\n",
    "    \"Sports\": f\"{BASE_URL}/sports\",\n",
    "    \"Entertainment\": f\"{BASE_URL}/entertainment\",\n",
    "    \"Science\": f\"{BASE_URL}/science\",\n",
    "    \"Economy\": f\"{BASE_URL}/business\"\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\"\n",
    "}\n",
    "\n",
    "all_articles = []\n",
    "\n",
    "for category, url in CATEGORY_URLS.items():\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.content, \"html.parser\")\n",
    "\n",
    "    articles_found = soup.find_all(\"h3\", class_=\"PagePromo-title\")\n",
    "    article_urls = []\n",
    "\n",
    "    for h3 in articles_found:\n",
    "        a_tag = h3.find(\"a\")\n",
    "        if a_tag and a_tag.get(\"href\"):\n",
    "            href = a_tag[\"href\"]\n",
    "            full_url = href if href.startswith(\"http\") else BASE_URL + href\n",
    "            article_urls.append(full_url)\n",
    "\n",
    "    article_urls = list(dict.fromkeys(article_urls))[:40]\n",
    "\n",
    "    for idx, url in enumerate(article_urls):\n",
    "        r = requests.get(url, headers=headers)\n",
    "        article_soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "\n",
    "        # title\n",
    "        title_tag = article_soup.find(\"h1\")\n",
    "        title = title_tag.get_text(strip=True) if title_tag else \"\"\n",
    "\n",
    "        # author\n",
    "        author = \"\"\n",
    "        byline = article_soup.find(\"div\", class_=\"Page-authors\")\n",
    "        if byline:\n",
    "            author_tag = byline.find(\"a\", class_=\"Link\")\n",
    "            if author_tag:\n",
    "                author = author_tag.get_text(strip=True).title()\n",
    "\n",
    "        # date\n",
    "        date = \"\"\n",
    "        date_container = article_soup.find(\"div\", class_=\"Page-dateModified\")\n",
    "        if date_container:\n",
    "            timestamp_tag = date_container.find(\"bsp-timestamp\")\n",
    "            if timestamp_tag and timestamp_tag.has_attr(\"data-timestamp\"):\n",
    "                timestamp_ms = int(timestamp_tag[\"data-timestamp\"])\n",
    "                dt = datetime.fromtimestamp(timestamp_ms / 1000)\n",
    "                date = dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        # content\n",
    "        content = \"\"\n",
    "        body = article_soup.find(\"div\", class_=\"RichTextStoryBody RichTextBody\")\n",
    "        if body:\n",
    "            paragraphs = body.find_all(\"p\")\n",
    "            content = \" \".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "        all_articles.append({\n",
    "            \"Title\": title,\n",
    "            \"URL\": url,\n",
    "            \"Source\": \"AP News\",\n",
    "            \"Journalist(s)\": author,\n",
    "            \"Published Date\": date,\n",
    "            \"Content\": content,\n",
    "            \"Category\": category,\n",
    "            \"Label\": 1 # real news\n",
    "        })\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "# save to csv\n",
    "df = pandas.DataFrame(all_articles)\n",
    "df.to_csv(\"apnews_real.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://techcrunch.com/\">TechCrunch</a> ü§ñ\n",
    "Specializes in technology and startup news. Since I am behind in the Technology Category, I decided to gather news from here to catch up.\n",
    "\n",
    "TechCrunch, founded in 2005, is an American online publisher focusing on the tech industry - particularly startups, venture capital, software and emerging technologies. Acquired by Yahoo (and later Verizon Media), TechCrunch is respected in the tech community for its specialized, forward-looking content and early coverage of startups that later become major industry players. While it does not have the same general-news scope as BBC or AP, it offers domain-specific accuracy in the rapidly evolving technology sector. Given a comparative shortage of technology-focused articles in the original dataset, TechCrunch was added to provide balance and relevance in the \"Technology\" category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://techcrunch.com'\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # article text\n",
    "    article_text = ''\n",
    "    paragraphs = soup.find_all('p', class_='wp-block-paragraph')\n",
    "    for p in paragraphs:\n",
    "        article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # date\n",
    "    publication_date = ''\n",
    "    time_element = soup.find('time', datetime=True)\n",
    "    if time_element:\n",
    "        publication_date = time_element['datetime'].split('T')[0]\n",
    "\n",
    "    # author\n",
    "    author = ''\n",
    "    author_link = soup.find('a', class_='wp-block-tc23-author-card-name__link')\n",
    "    if author_link:\n",
    "        author = author_link.get_text(strip=True)\n",
    "\n",
    "    return article_text, publication_date, author\n",
    "\n",
    "def scrape_techcrunch_articles(pages=5):\n",
    "    articles = []\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f'{base_url}/latest/page/{page}/'\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        article_count = 0\n",
    "        for article in soup.find_all('div', class_='wp-block-techcrunch-card wp-block-null'):\n",
    "            title_tag = article.find('h3', class_='loop-card__title')\n",
    "            title = title_tag.get_text(strip=True) if title_tag else ''\n",
    "\n",
    "            link_tag = title_tag.find('a', class_='loop-card__title-link') if title_tag else None\n",
    "            href = link_tag['href'] if link_tag else ''\n",
    "\n",
    "            # link is a full URL\n",
    "            link = href if href.startswith('http') else f\"{base_url}{href}\"\n",
    "\n",
    "            # article details\n",
    "            article_text, publication_date, author = scrape_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'Title': title,\n",
    "                'URL': link,\n",
    "                'Source': \"TechCrunch\",\n",
    "                'Journalist(s)': author,\n",
    "                'Published Date': publication_date,\n",
    "                'Content': article_text,\n",
    "                'Category': 'Technology',\n",
    "                'Label': 1  # real news\n",
    "            })\n",
    "            article_count += 1\n",
    "\n",
    "    return articles\n",
    "\n",
    "# 5 pages\n",
    "techcrunch_articles = scrape_techcrunch_articles(pages=5)\n",
    "\n",
    "# save to csv\n",
    "df = pandas.DataFrame(techcrunch_articles)\n",
    "df.to_csv(\"techcrunch_real.csv\", index=False)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
