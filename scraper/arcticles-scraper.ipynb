{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping ‚õèÔ∏è\n",
    "\n",
    "Let's begin by importing the libraries and checking their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 1.6.1\n",
      "pandas version: 2.2.3\n",
      "seaborn version: 0.13.2\n",
      "requests version: 2.31.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas\n",
    "import seaborn\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"scikit-learn version:\", sklearn.__version__)     # 1.6.1\n",
    "print(\"pandas version:\", pandas.__version__)            # 2.2.3\n",
    "print(\"seaborn version:\", seaborn.__version__)          # 0.13.2\n",
    "print(\"requests version:\", requests.__version__)        # 2.31.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article Details üîé\n",
    "I will examine each article's \n",
    "- Title \n",
    "- Link \n",
    "- Author (if available)\n",
    "- Publication Date\n",
    "- Content/text\n",
    "- Categorize it as \"Fake\" (0) or \"Real\" (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBC üåê\n",
    "\n",
    "I believe that starting with the BBC would be the most straightforward option, as my research shows they are ranked as the most trusted news source by Americans. Even though we live in Europe, I choose to trust this statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to bbc_news_articles_v2.csv\n"
     ]
    }
   ],
   "source": [
    "bbc_url = 'https://www.bbc.com'\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # article text\n",
    "    article_text = ''\n",
    "    article_body = soup.find('article')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')  #find all p tags\n",
    "        for p in paragraphs:\n",
    "            article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # author\n",
    "    author_tag = soup.find('span', class_='sc-b42e7a8f-7 kItaYD')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else ''\n",
    "\n",
    "    # publication date\n",
    "    time_tag = soup.find('time', {'datetime': True})\n",
    "    publication_date = time_tag['datetime'] if time_tag else ''\n",
    "\n",
    "    return article_text, author, publication_date\n",
    "\n",
    "def scrape_news():\n",
    "    url = f'{bbc_url}/news'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles = []\n",
    "    for article in soup.find_all('a', class_='sc-2e6baa30-0 gILusN'):\n",
    "        title_element = article.find('h2', class_='sc-87075214-3 eywmDE')\n",
    "        if title_element:\n",
    "            title = title_element.get_text(strip=True)\n",
    "            href = article['href']\n",
    "\n",
    "            # check if href is a full URL or relative path\n",
    "            if href.startswith('http'):\n",
    "                link = href\n",
    "            else:\n",
    "                link = f\"{bbc_url}{href}\"\n",
    "\n",
    "            # additional details\n",
    "            article_text, author, publication_date = scrape_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'author': author,\n",
    "                'date': publication_date,\n",
    "                'text': article_text,\n",
    "                'classification': 1\n",
    "            })\n",
    "\n",
    "    return articles\n",
    "\n",
    "news_articles = scrape_news()\n",
    "\n",
    "# save to CSV\n",
    "df = pandas.DataFrame(news_articles)\n",
    "df.to_csv(\"bbc_news_articles_v2.csv\", index=False)\n",
    "\n",
    "print(\"Saved to bbc_news_articles_v2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other dataset\n",
    "\n",
    "For now, I have decided to use another dataset I found on Hugging Face (https://huggingface.co/datasets/ErfanMoosaviMonazzah/fake-news-detection-dataset-English). Although the data is 6-7 years old and may not be fully relevant today, it contains valuable information that fits my current assignment. Since this is the initial stage of the project ‚Äî Iteration 0 ‚Äî I will start with this dataset, and later on, I plan to enhance it by scraping more relevant sources. The dataset includes both fake and real news, but it does not provide links. For the time being, I will train the model to work with the text and titles, with the intention of improving it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29999, 6)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_dataset = pandas.read_csv('huggingface_dataset.csv')\n",
    "bbc_dataset.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
