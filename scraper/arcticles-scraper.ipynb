{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping ⛏️\n",
    "First, lets begin by importing the libraries, we are going to use today and checking their versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scikit-learn version: 1.6.1\n",
      "pandas version: 2.2.3\n",
      "seaborn version: 0.13.2\n",
      "requests version: 2.31.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import pandas\n",
    "import seaborn\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"scikit-learn version:\", sklearn.__version__)     # 1.6.1\n",
    "print(\"pandas version:\", pandas.__version__)            # 2.2.3\n",
    "print(\"seaborn version:\", seaborn.__version__)          # 0.13.2\n",
    "print(\"requests version:\", requests.__version__)        # 2.31.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I plan to examine each article's:\n",
    "- title\n",
    "- link\n",
    "- source (which is BBC)\n",
    "- author (if available, otherwise - unknown)\n",
    "- publication date\n",
    "- text/content\n",
    "- \"Fake\" (0) or \"Real\" (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truthful websites\n",
    "Since this is my first attempt at scraping news data, I will begin by examining trustworthy news outlets.\n",
    "### BBC\n",
    "I believe that starting with the BBC would be the easiest, as I did my research and found out that they were ranked as the most trusted news source by Americans. Even though we live in Europe, I choose to believe this statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to bbc_news_articles_v1.csv\n"
     ]
    }
   ],
   "source": [
    "bbc_url = 'https://www.bbc.com'\n",
    "\n",
    "def scrape_article_details(link):\n",
    "    response = requests.get(link)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # article text\n",
    "    article_text = ''\n",
    "    article_body = soup.find('article')\n",
    "    if article_body:\n",
    "        paragraphs = article_body.find_all('p')  #find all p tags\n",
    "        for p in paragraphs:\n",
    "            article_text += p.get_text(strip=True) + '\\n\\n'\n",
    "\n",
    "    # author\n",
    "    author_tag = soup.find('span', class_='sc-b42e7a8f-7 kItaYD')\n",
    "    author = author_tag.get_text(strip=True) if author_tag else ''\n",
    "\n",
    "    # publication date\n",
    "    time_tag = soup.find('time', {'datetime': True})\n",
    "    publication_date = time_tag['datetime'] if time_tag else ''\n",
    "\n",
    "    return article_text, author, publication_date\n",
    "\n",
    "def scrape_news():\n",
    "    url = f'{bbc_url}/news'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    articles = []\n",
    "    for article in soup.find_all('a', class_='sc-2e6baa30-0 gILusN'):\n",
    "        title_element = article.find('h2', class_='sc-87075214-3 eywmDE')\n",
    "        if title_element:\n",
    "            title = title_element.get_text(strip=True)\n",
    "            href = article['href']\n",
    "\n",
    "            # check if href is a full URL or relative path\n",
    "            if href.startswith('http'):\n",
    "                link = href\n",
    "            else:\n",
    "                link = f\"{bbc_url}{href}\"\n",
    "\n",
    "            # additional details\n",
    "            article_text, author, publication_date = scrape_article_details(link)\n",
    "\n",
    "            articles.append({\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'source': 'BBC',\n",
    "                'author': author,\n",
    "                'date': publication_date,\n",
    "                'text': article_text,\n",
    "                'classification': 1\n",
    "            })\n",
    "\n",
    "    return articles\n",
    "\n",
    "news_articles = scrape_news()\n",
    "\n",
    "# save to CSV\n",
    "df = pandas.DataFrame(news_articles)\n",
    "df.to_csv(\"bbc_news_articles_v1.csv\", index=False)\n",
    "\n",
    "print(\"Saved to bbc_news_articles_v1.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
